[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS-5805-Machine-Learning-Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLinear and Non Linear Regression\n\n\n\n\n\n\n\nLinear regression\n\n\nNon linear regression\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nRahul Satoskar\n\n\n\n\n\n\n  \n\n\n\n\nK means clustering on Iris dataset\n\n\n\n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nRahul Satoskar\n\n\n\n\n\n\n  \n\n\n\n\nIris Dataset Classification using Machine Learning\n\n\n\n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2023\n\n\nRahul Satoskar\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly Detection using DBSCAN\n\n\n\n\n\n\n\nAnomaly/outlier detection\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nRahul Satoskar\n\n\n\n\n\n\n  \n\n\n\n\nAssumptions in Linear Regression\n\n\n\n\n\n\n\nProbability theory and random variables\n\n\nLinear regression\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nRahul Satoskar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Rahul Satoskar"
  },
  {
    "objectID": "posts/dbscan_anomaly_detection/anomaly_detection_dbscan_final.html",
    "href": "posts/dbscan_anomaly_detection/anomaly_detection_dbscan_final.html",
    "title": "CS-5805-Machine-Learning-Blogs",
    "section": "",
    "text": "References used\nhttps://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\nhttps://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\n# Reference- https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html\n\n# Let us now create a random dataset consisting of isotropic Gaussian blobs for clustering by using the make_blob() function.\n# Visualization of our random dataset\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(n_samples=300, centers=1, random_state=20, cluster_std=.4, center_box=(20, 6))\nplt.scatter(X[:,0], X[:,1])\nplt.show()\n\n\n\n\nFig 1- Plotting our randomly generated data\n\n\n\n\n\n# We will fit the DBSCAN model with our input data X and make predictons on it\n# Note noisy samples or outlier points are given the label -1.\npred_labels = DBSCAN(eps=0.3, min_samples=25).fit_predict(X)\n\n\n# Getting the index of the outlier points\noutliers_index = []\nfor i in range(len(pred_labels)):\n    if pred_labels[i] == -1:\n        outliers_index.append(i)\n        \n# Displaying the index of outlier points\noutliers_index     \n\n[16, 20, 21, 29, 73, 75, 174, 176, 203, 220, 246, 287]\n\n\n\n# Outlier points\noutliers = X[outliers_index]\noutliers\n\narray([[11.11436818,  6.70590549],\n       [11.23666724,  6.51744854],\n       [11.80089649,  6.4258661 ],\n       [11.69871034,  6.15612669],\n       [12.51273803,  6.64052922],\n       [12.4970254 ,  6.9974063 ],\n       [11.96354368,  6.35026171],\n       [10.7614565 ,  7.26509908],\n       [11.90918339,  6.49470305],\n       [12.70384828,  7.61549344],\n       [10.98141189,  6.69119251],\n       [10.93877139,  7.96542482]])\n\n\n\n# Let us now visualize our data and mark the outlier points as red\nplt.scatter(X[:,0], X[:,1])\nplt.scatter(outliers[:,0], outliers[:,1], color='r')\n\n&lt;matplotlib.collections.PathCollection at 0x1f24c1aa610&gt;\nFig 2- Plotting our randomly generated data and marking our outlier points as red"
  },
  {
    "objectID": "posts/dbscan_anomaly_detection/index.html",
    "href": "posts/dbscan_anomaly_detection/index.html",
    "title": "Anomaly Detection using DBSCAN",
    "section": "",
    "text": "In this blog we will talk about anomaly detection using DBSCAN algorithm."
  },
  {
    "objectID": "posts/dbscan_anomaly_detection/index.html#dbscan-terms",
    "href": "posts/dbscan_anomaly_detection/index.html#dbscan-terms",
    "title": "Anomaly Detection using DBSCAN",
    "section": "DBSCAN terms",
    "text": "DBSCAN terms\n1- Density at point p- The number of points within a circle of radius Eps.\n2- Dense Region- A circle of radius Eps which contains atleast MinPts points.\n3- Core Points- A point is a core point if it has more than MinPts within Eps radius around it.\n4- Border point- A point is a border point if it has fewer than MinPts within Eps radius around it but is in the neighborhood of a core point.\n5- Noise point- A noise point is a point which is neither a core point or a border point\nIn this blog we will find these noise points or outliers using the DBSCSAN algorithm."
  },
  {
    "objectID": "posts/iris_classification/index.html",
    "href": "posts/iris_classification/index.html",
    "title": "Iris Dataset Classification using Machine Learning",
    "section": "",
    "text": "Introduction\nIn this blog we will cover Iris flower classification using supervised machine learning algorithms namely Logistic regression and Random Forest. Iris data set is a popular data set and is useful for people who have started in their data science journey. Iris data set contains feastures of different flower species namely Sepal Length, Sepal Width, Petal Length and Petal Width and is used to classify flowers into 3 species namely Sentosa, Versicolor and Virginica.\n\n\nLogistic Regression\nLogistic regression is a supervised machine learning model and is used for classification tasks. Logistic regression is used to predict the probability of a binary outcome. For a binary classification task the goal of logistic regression is to find a hyper plane which best separates positive and negative points.\n\n\nRandom Forest\nRandom Forest is a supervised learning ensemble based technique which can be used for both classification and regression tasks. The base model of a random forest is a decision tree and a random forest is an ensemble of multiple decision trees. Random forest is a non linear model and works well for non linear relationships. It does a good job in handling outliers and missing values.\n\n\nIris Flower classification steps\nWe start with extracting the iris data set and converting it into a pandas data frame as shown below\n\n\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\niris = datasets.load_iris()\n\n# Converting Iris data into a pandas data frame\niris_df = pd.DataFrame(data=iris.data,  \n                  columns=iris.feature_names)\n\niris_df['species'] = iris.target\nprint(\"Different Target species of Iris dataset are \", iris.target_names)\n  \n# Display dataframe\niris_df\n\nDifferent Target species of Iris dataset are  ['setosa' 'versicolor' 'virginica']\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n2\n\n\n146\n6.3\n2.5\n5.0\n1.9\n2\n\n\n147\n6.5\n3.0\n5.2\n2.0\n2\n\n\n148\n6.2\n3.4\n5.4\n2.3\n2\n\n\n149\n5.9\n3.0\n5.1\n1.8\n2\n\n\n\n\n150 rows × 5 columns\n\nFig 1- Iris Dataset\n\n\nSource: iris_classification_final.ipynb\nNext we check whether our data is balanced or imbalanced by plotting the class counts as shown below.\n\n\n# Plotting the class counts of target species\nsns.countplot(iris_df['species']);\n\n\n\n\nFig 2- Iris Dataset species value counts\n\n\n\n\nSource: iris_classification_final.ipynb\nWe can see from above that our Iris data is a balanced data.\nNext we spit the features and target variables and divide the data into training and test data as shown below.\n\n\n# Splitting features and target variables\n\nX= iris_df.iloc[:,0:4]\ny=iris_df.iloc[:,4]\n\n# Splitting the dataset into train and test datasets\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=5)\n\nSource: iris_classification_final.ipynb\nNext we train a logistic regression model on our data as shown below.\n\n\n# Let us now use supervised learning algorithms like Logistic regression for the classification task.\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\n# Fitting Logistic regression on the train data\nLR.fit(X_train,y_train)\n#Predicting the output on test data using the trained Logistic regression model\ny_pred=LR.predict(X_test)\n\nSource: iris_classification_final.ipynb\nNext we display the classification report of logistic regression which proves that logistic regression performs well on the Iris data set.\n\n\n#Displaying classification metrics for Logistic regression\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,   y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        12\n           1       1.00      0.93      0.96        14\n           2       0.92      1.00      0.96        12\n\n    accuracy                           0.97        38\n   macro avg       0.97      0.98      0.97        38\nweighted avg       0.98      0.97      0.97        38\n\n\n\nSource: iris_classification_final.ipynb\nLet us now see the accuracy of our logistic regression model.\n\n\n# Printing accuracy for Logistic regression\naccuracy=accuracy_score(y_test,y_pred)*100\nprint(\"Accuracy of Logistic Regression model is \", accuracy)\n\nAccuracy of Logistic Regression model is  97.36842105263158\n\n\nSource: iris_classification_final.ipynb\nWe now plot the heat map of the confusion matrix of logistic regression as shown below.\n\n\n# Plotting heatmap of confusion matrix for Logistic regression\n\nimport seaborn as sns\n\ncm  = confusion_matrix(y_test, y_pred) \n\ncm_df = pd.DataFrame(cm,index = ['setosa','versicolor','virginica'], columns = ['setosa','versicolor','virginica'])\n\nplt.figure(figsize=(6,4))\n\nsns.heatmap(cm_df,   annot=True)\n\nplt.ylabel('True value')\n\nplt.xlabel('Predicted value')\n\nplt.show()\n\n\n\n\nFig 3- Heatmap of confusion matrix for Logistic regression\n\n\n\n\nSource: iris_classification_final.ipynb\nThe heat map shows that Logistic regression is doing a good job in iris data classification.\nNext we train a Random Forest model on our Iris data set as shown below.\n\n\n# Let us now use non lineaar supervised learning algorithms like Random forest which is an ensemble model for the classification task.\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(random_state=5)\n# Fitting Random Forest on the train data\nRF.fit(X_train,y_train)\n#Predicting the output on test data using the trained Random forest model\ny_pred=RF.predict(X_test)\ny_pred\n\narray([1, 1, 2, 0, 2, 1, 0, 2, 0, 1, 1, 1, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2,\n       0, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 0, 2])\n\n\nSource: iris_classification_final.ipynb\nNext we display the classification metrics for random forest which shows that logistic regression is performing better random forest for iris data classification.\nThe below code shows that the accuracy of random forest is less than that of logistic regression.\n\n\n# Printing accuracy\naccuracy=accuracy_score(y_test,y_pred)*100\nprint(\"Accuracy of Random forest model is \", accuracy)\n\nAccuracy of Random forest model is  92.10526315789474\n\n\nSource: iris_classification_final.ipynb\nNext we display the heat map of the confusion matrix of Random forest which shows that logistic regression performs better than Logistic regression for the Iris data set.\n\n\n# Plotting heatmap of confusion matrix for Random Forest\n\nimport seaborn as sns\n\ncm  = confusion_matrix(y_test, y_pred) \n\ncm_df = pd.DataFrame(cm,index = ['setosa','versicolor','virginica'], columns = ['setosa','versicolor','virginica'])\n\nplt.figure(figsize=(6,4))\n\nsns.heatmap(cm_df,   annot=True)\n\nplt.ylabel('True value')\n\nplt.xlabel('Predicted value')\n\nplt.show()\n\n\n\n\nFig 4- Heatmap of confusion matrix for Random forest\n\n\n\n\nSource: iris_classification_final.ipynb\n\n\nConclusion\nIn this blog we learnt how to use machine learning models like logistic regression and random forest to classify an iris flower data set. It is to be noted that a complex model doesn’t necessary give better performance metrics when compared to a relatively simpler model as shown by the performance metrics of Logistic regression and Random forest on the Iris data set.\n\n\nPython notebook code link in the Github repository\nhttps://github.com/rahulsatoskar/CS-5805-Machine-Learning-Blogs/blob/main/Python%20notebooks/iris_classification_final.ipynb"
  },
  {
    "objectID": "posts/iris_classification/iris_classification_final.html",
    "href": "posts/iris_classification/iris_classification_final.html",
    "title": "CS-5805-Machine-Learning-Blogs",
    "section": "",
    "text": "from sklearn import datasets\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\niris = datasets.load_iris()\n\n# Converting Iris data into a pandas data frame\niris_df = pd.DataFrame(data=iris.data,  \n                  columns=iris.feature_names)\n\niris_df['species'] = iris.target\nprint(\"Different Target species of Iris dataset are \", iris.target_names)\n  \n# Display dataframe\niris_df\n\nDifferent Target species of Iris dataset are  ['setosa' 'versicolor' 'virginica']\n\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n0\n\n\n1\n4.9\n3.0\n1.4\n0.2\n0\n\n\n2\n4.7\n3.2\n1.3\n0.2\n0\n\n\n3\n4.6\n3.1\n1.5\n0.2\n0\n\n\n4\n5.0\n3.6\n1.4\n0.2\n0\n\n\n...\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n2\n\n\n146\n6.3\n2.5\n5.0\n1.9\n2\n\n\n147\n6.5\n3.0\n5.2\n2.0\n2\n\n\n148\n6.2\n3.4\n5.4\n2.3\n2\n\n\n149\n5.9\n3.0\n5.1\n1.8\n2\n\n\n\n\n150 rows × 5 columns\n\nFig 1- Iris Dataset\n\n\n\n# Checking if the data is balanced or imbalanced\niris_df['species'].value_counts()\n\n0    50\n1    50\n2    50\nName: species, dtype: int64\n\n\n\n# Plotting the class counts of target species\nsns.countplot(iris_df['species']);\n\n\n\n\nFig 2- Iris Dataset species value counts\n\n\n\n\n\n# Splitting features and target variables\n\nX= iris_df.iloc[:,0:4]\ny=iris_df.iloc[:,4]\n\n# Splitting the dataset into train and test datasets\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,random_state=5)\n\n\n# Let us now use supervised learning algorithms like Logistic regression for the classification task.\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\n# Fitting Logistic regression on the train data\nLR.fit(X_train,y_train)\n#Predicting the output on test data using the trained Logistic regression model\ny_pred=LR.predict(X_test)\n\n\n#Displaying classification metrics for Logistic regression\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,   y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        12\n           1       1.00      0.93      0.96        14\n           2       0.92      1.00      0.96        12\n\n    accuracy                           0.97        38\n   macro avg       0.97      0.98      0.97        38\nweighted avg       0.98      0.97      0.97        38\n\n\n\n\n# Printing confusion matrix for Logistic regression\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nconfusion_matrix(y_test,y_pred)\n\narray([[12,  0,  0],\n       [ 0, 13,  1],\n       [ 0,  0, 12]], dtype=int64)\n\n\n\n# Printing accuracy for Logistic regression\naccuracy=accuracy_score(y_test,y_pred)*100\nprint(\"Accuracy of Logistic Regression model is \", accuracy)\n\nAccuracy of Logistic Regression model is  97.36842105263158\n\n\n\n# Plotting heatmap of confusion matrix for Logistic regression\n\nimport seaborn as sns\n\ncm  = confusion_matrix(y_test, y_pred) \n\ncm_df = pd.DataFrame(cm,index = ['setosa','versicolor','virginica'], columns = ['setosa','versicolor','virginica'])\n\nplt.figure(figsize=(6,4))\n\nsns.heatmap(cm_df,   annot=True)\n\nplt.ylabel('True value')\n\nplt.xlabel('Predicted value')\n\nplt.show()\n\n\n\n\nFig 3- Heatmap of confusion matrix for Logistic regression\n\n\n\n\n\n# Let us now use non lineaar supervised learning algorithms like Random forest which is an ensemble model for the classification task.\nfrom sklearn.ensemble import RandomForestClassifier\nRF = RandomForestClassifier(random_state=5)\n# Fitting Random Forest on the train data\nRF.fit(X_train,y_train)\n#Predicting the output on test data using the trained Random forest model\ny_pred=RF.predict(X_test)\ny_pred\n\narray([1, 1, 2, 0, 2, 1, 0, 2, 0, 1, 1, 1, 2, 2, 0, 0, 2, 2, 0, 0, 1, 2,\n       0, 1, 1, 2, 1, 1, 1, 2, 0, 1, 1, 0, 1, 0, 0, 2])\n\n\n\n#Displaying classification metrics for Random forest\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test,   y_pred))\n\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00        12\n           1       0.87      0.93      0.90        14\n           2       0.91      0.83      0.87        12\n\n    accuracy                           0.92        38\n   macro avg       0.93      0.92      0.92        38\nweighted avg       0.92      0.92      0.92        38\n\n\n\n\n# Printing confusion matrix\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nconfusion_matrix(y_test,y_pred)\n\narray([[12,  0,  0],\n       [ 0, 13,  1],\n       [ 0,  2, 10]], dtype=int64)\n\n\n\n# Printing accuracy\naccuracy=accuracy_score(y_test,y_pred)*100\nprint(\"Accuracy of Random forest model is \", accuracy)\n\nAccuracy of Random forest model is  92.10526315789474\n\n\n\n# Plotting heatmap of confusion matrix for Random Forest\n\nimport seaborn as sns\n\ncm  = confusion_matrix(y_test, y_pred) \n\ncm_df = pd.DataFrame(cm,index = ['setosa','versicolor','virginica'], columns = ['setosa','versicolor','virginica'])\n\nplt.figure(figsize=(6,4))\n\nsns.heatmap(cm_df,   annot=True)\n\nplt.ylabel('True value')\n\nplt.xlabel('Predicted value')\n\nplt.show()\n\n\n\n\nFig 4- Heatmap of confusion matrix for Random forest"
  },
  {
    "objectID": "posts/iris_clustering/index.html",
    "href": "posts/iris_clustering/index.html",
    "title": "K means clustering on Iris dataset",
    "section": "",
    "text": "In this blog we will talk about using K-means clustering on Iris data set.\n\nUnsupervised Learning\nUnsupervised learning works with unlabeled data where the algorithm finds patters and relationships in the data on its own without any guidance.\nUnsupervised learning is used in clustering and dimensionality reduction. Clustering is used for grouping similar data points in the form of clusters such that points belonging to one cluster are homogeneous to each other compared to points belonging to other clusters. Dimensionality reduction is used to limit the numbers of features in our data set while preserving as much information as possible in the process.\n\n\nK- means clustering\nK-means clustering is used to find groups or clusters in the data with the number of clusters denoted by the variable K- means clustering is an iterative algorithm where each point is assigned to one of the clusters and this process repeats until final clusters are formed.\nAfter running K-means on a data set we get the following outputs:\na- K centroids- There are the centroids for each of the K clusters.\nb- We get labels for the training data which denotes which points belongs to which clusters.\n\n\nIris data set\nThis data set contains 3 classes of flowers namely Iris Sentosa, Iris Versicolour and Iris Virginica. The features in this data set are sepal length, sepal width, petal length and petal width. This is one of the first and basic data sets which students encounter when they start learning machine learning. We will be using K means which is an unsupervised machine learning algorithm to predict the flower classes.\nIn the code below we display features of the Iris data set.\n\n\nfrom sklearn import datasets\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(\"ignore\")\nfrom sklearn.cluster import KMeans\n\niris = datasets.load_iris()\n\n# Converting Iris data into a pandas data frame\niris_df = pd.DataFrame(data=iris.data,  \n                  columns=iris.feature_names)\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\nFig 1- Iris Dataset\n\n\nSource: iris_clustering.ipynb\nBelow code shows how K-means is initialized using sklearn.\n\n\n# Using K means clustering on Iris dataset\nkmeans = KMeans(n_clusters=3,init = 'k-means++', random_state = 20)\nkmeans\n\nKMeans(n_clusters=3, random_state=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3, random_state=20)\n\n\nSource: iris_clustering.ipynb\nThe below code shows for each data point to which cluster it belongs to.\n\n\n# Print cluster centers\nprint(kmeans.cluster_centers_) \n\n[[6.85       3.07368421 5.74210526 2.07105263]\n [5.006      3.428      1.462      0.246     ]\n [5.9016129  2.7483871  4.39354839 1.43387097]]\n\n\nSource: iris_clustering.ipynb\nFinally we perform K-means clustering on our data set and the assigned clusters are shown in color as shown below.\n\n\n# Visualizing the clusters formed by K means on Iris dataset\n\nplt.scatter(X[y_pred   == 0, 0], X[y_pred == 0, 1],s = 100, c = 'red', label = 'Iris-setosa')\n\nplt.scatter(X[y_pred   == 1, 0], X[y_pred == 1, 1],s = 100, c = 'blue', label = 'Iris-versicolour')\n\nplt.scatter(X[y_pred   == 2, 0], X[y_pred == 2, 1],s = 100, c = 'green', label = 'Iris-virginica') \n\n# Centroids of each of the clusters is shown in black\nplt.scatter(kmeans.cluster_centers_[:,   0], kmeans.cluster_centers_[:,1],s = 100, c = 'black', label = 'Centroids')   \n\nplt.legend()\n\nplt.show()\n\n\n\n\nFig 2- Visualizing the clusters formed by K means on Iris dataset\n\n\n\n\nSource: iris_clustering.ipynb\nObservations- We can see that K means has formed a well separated cluster for versicolor while there is overlapping between clusters of virginica and sentosa.\n\n\nConclusion-\nWe have successfully performed K-means clustering on the Iris data set.\n\n\nLink of Python notebook in Gitgub Repository-\nhttps://github.com/rahulsatoskar/CS-5805-Machine-Learning-Blogs/blob/main/Python%20notebooks/iris_clustering.ipynb"
  },
  {
    "objectID": "posts/iris_clustering/iris_clustering.html",
    "href": "posts/iris_clustering/iris_clustering.html",
    "title": "CS-5805-Machine-Learning-Blogs",
    "section": "",
    "text": "from sklearn import datasets\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(\"ignore\")\nfrom sklearn.cluster import KMeans\n\niris = datasets.load_iris()\n\n# Converting Iris data into a pandas data frame\niris_df = pd.DataFrame(data=iris.data,  \n                  columns=iris.feature_names)\niris_df\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\n\n\n1\n4.9\n3.0\n1.4\n0.2\n\n\n2\n4.7\n3.2\n1.3\n0.2\n\n\n3\n4.6\n3.1\n1.5\n0.2\n\n\n4\n5.0\n3.6\n1.4\n0.2\n\n\n...\n...\n...\n...\n...\n\n\n145\n6.7\n3.0\n5.2\n2.3\n\n\n146\n6.3\n2.5\n5.0\n1.9\n\n\n147\n6.5\n3.0\n5.2\n2.0\n\n\n148\n6.2\n3.4\n5.4\n2.3\n\n\n149\n5.9\n3.0\n5.1\n1.8\n\n\n\n\n150 rows × 4 columns\n\nFig 1- Iris Dataset\n\n\n\n# Converting Pandas data frame to numpy array\nX= iris_df.values\n\n\n# Using K means clustering on Iris dataset\nkmeans = KMeans(n_clusters=3,init = 'k-means++', random_state = 20)\nkmeans\n\nKMeans(n_clusters=3, random_state=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=3, random_state=20)\n\n\n\n# Compute cluster centers and predict cluster index for each sample.\ny_pred = kmeans.fit_predict(X)\n\ny_pred\n\narray([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 2, 0, 0, 0,\n       0, 0, 0, 2, 2, 0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 2, 2, 0, 0, 0, 0,\n       0, 2, 0, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 2])\n\n\n\n# Print cluster centers\nprint(kmeans.cluster_centers_) \n\n[[6.85       3.07368421 5.74210526 2.07105263]\n [5.006      3.428      1.462      0.246     ]\n [5.9016129  2.7483871  4.39354839 1.43387097]]\n\n\n\n# Visualizing the clusters formed by K means on Iris dataset\n\nplt.scatter(X[y_pred   == 0, 0], X[y_pred == 0, 1],s = 100, c = 'red', label = 'Iris-setosa')\n\nplt.scatter(X[y_pred   == 1, 0], X[y_pred == 1, 1],s = 100, c = 'blue', label = 'Iris-versicolour')\n\nplt.scatter(X[y_pred   == 2, 0], X[y_pred == 2, 1],s = 100, c = 'green', label = 'Iris-virginica') \n\n# Centroids of each of the clusters is shown in black\nplt.scatter(kmeans.cluster_centers_[:,   0], kmeans.cluster_centers_[:,1],s = 100, c = 'black', label = 'Centroids')   \n\nplt.legend()\n\nplt.show()\n\n\n\n\nFig 2- Visualizing the clusters formed by K means on Iris dataset\n\n\n\n\nObservations- We can see that K means has formed a well separared cluster for versicolor while there is overlapping between clusters of virginica and sentosa."
  },
  {
    "objectID": "posts/linear and non linear regression/index.html",
    "href": "posts/linear and non linear regression/index.html",
    "title": "Linear and Non Linear Regression",
    "section": "",
    "text": "In this blog we will talk about Linear and Non Linear Regression. We will plot graphs to show for which data is linear regression suitable and for which data is non linear regression suitable.\n\nLinear regression\nLinear regression is a machine learning model which models the relationship between a dependent variable and one or more independent variables. Linear regression assumes that the relationship between the dependent variable and independent variables is linear and it can be represented by a straight line. Linear regression is represented by the equation as shown below:\ny = a0 + a1x1 + a2x2 + … + anxn + Error term\nwhere y is the dependent variable and a0, a1 etc are the coefficients. These coefficients are estimated from the data using methods such as ordinary least squares which minimizes the sum of squared errors. The error term is not explained by the linear model and it represents the random variation.\nLinear regression is suitable for capturing relationships in the data if the data is linear. It is not suitable for non linear or complex data.\nLinear regression is sensitive to outliers and it can distort the results of the trained linear regression fitted line.\n\n\nNon linear regression\nIn non linear regression a dependent variable is a non linear combination of one or more independent variables. Non linear regression can fit complex curves which are not linear in nature like for example quadratic curves or polynomials with higher degrees. While linear regression models have a unique solution non linear regression models can have multiple local minima.\n\n\nFitting linear regression on different kinds of data\nLet us first import python libraries as shown below.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nSource: linear_non_linear_regression_final.ipynb\nNext we generate a random data with some Gaussian noise which has a linear relationship and fit a linear regression model to it. The fitted linear regression line is as shown below which shows that linear regression is doing a good job on this data.\n\n\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = np.random.rand(150,1)\n\n# Let us generate a function y which is a linear function of X with added Gaussian noise\ny = 10 * X + 5 + np.random.randn(150, 1)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X, y)\n\n# https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\nX1 = np.linspace(0, 1, 150).reshape(-1, 1)\ny1_pred = LR.predict(X1)\n\nplt.scatter(X, y)\n# Let us plot a linear regression line for the points X in red\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 1- Linear regression visualization\n\n\n\n\nSource: linear_non_linear_regression_final.ipynb\nNext we plot the fitted linear regression line as shown below where the randomly generated data has more noise and we can see that linear regression does not do a good job in this case.\n\n\n# Adding more noise to our data\n# Linear regression visualization with more noise in our data\n\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = np.random.rand(150,1)\n\n# Let us generate a function y which is a linear function of X with added Gaussian noise\ny = 10 * X + 5 + 5 * np.random.randn(150, 1)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X, y)\n\nX1 = np.linspace(0, 1, 150).reshape(-1, 1)\ny1_pred = LR.predict(X1)\n\nplt.scatter(X, y)\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 2- Linear regression visualization with more noise\n\n\n\n\nSource: linear_non_linear_regression_final.ipynb\nNext we plot the fitted linear regression line as shown below where the randomly generated data has less noise and we can see that linear regression does an excellent job in this case.\n\n\n# Adding less noise to our data\n# Linear regression visualization with less noise in our data\n\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = np.random.rand(150,1)\n\n# Let us generate a function y which is a linear function of X with added Gaussian noise\ny = 10 * X + 5 + 0.2 * np.random.randn(150, 1)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X, y)\n\nX1 = np.linspace(0, 1, 150).reshape(-1, 1)\ny1_pred = LR.predict(X1)\n\nplt.scatter(X, y)\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 3- Linear regression visualization with less noise\n\n\n\n\nSource: linear_non_linear_regression_final.ipynb\nNext we plot a linear regression line with quadratic data as shown below. As we can see the line does not do a good job in fitting a quadratic curve amd hence a linear model like linear regression is not good for fitting such a quadratic data.\nThis is why we want to have polynomial regression to fit such a data. For example quadratic regression would be able to fit such a quadratic data.\nFor such a non linear data like the quadratic data as shown above we use polynomial features where we take the features which we have and we create polynomial features based on those features.\n\n\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = 4 * np.random.rand(80,1) - 2\n\n# Let us generate a function y which is a quadratic function of X with added Gaussian noise\ny = 2 * X + 5 + X**2 + np.random.randn(80, 1)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X, y)\n\n# https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\nX1 = np.linspace(-2, 2, 80).reshape(-1, 1)\ny1_pred = LR.predict(X1)\n\nplt.scatter(X, y)\n# Let us plot a linear regression line for the points X in red\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 4- Linear regression visualization with quadratic data\n\n\n\n\nSource: linear_non_linear_regression_final.ipynb\nNext we use PolynomialFeatures function of sklearn to generate quadratic features to train a linear regression model to make predictions on quadratic data as shown below.\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = 4 * np.random.rand(80,1) - 2\n\n# Let us generate a function y which is a quadratic function of X with added Gaussian noise\ny = 2 * X + 5 + X**2 + np.random.randn(80, 1)\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X_poly, y)\n\n# https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\nX1 = np.linspace(-2, 2, 80).reshape(-1, 1)\nX1_poly = poly.fit_transform(X1)\n\ny1_pred = LR.predict(X1_poly)\n\nplt.scatter(X, y)\n# Let us plot a linear regression line for the points X in red\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 5- Linear regression visualization with quadratic data and quadratic features used to train linear regression model\n\n\n\n\nSource: linear_non_linear_regression_final.ipynb\nNow we can see we have a quadratic function to fit these points and quadratic linear regression does a better job than linear regression.\nNext we use PolynomialFeatures function of sklearn to generate degree 4 features to train linear regression on a complex data set as shown below.\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = 4 * np.random.rand(80,1) - 2\n\n# Let us generate a function y which has degree 1, quadratic, cubic and degree 4 terms of X with added Gaussian noise\ny = 2 * X + 5 + X**2 + 10 * X**3 + 2 * X**4 + 5 * np.random.randn(80, 1)\n\npoly = PolynomialFeatures(degree=4, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X_poly, y)\n\n# https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\nX1 = np.linspace(-2, 2, 80).reshape(-1, 1)\nX1_poly = poly.fit_transform(X1)\n\ny1_pred = LR.predict(X1_poly)\n\nplt.scatter(X, y)\n# Let us plot a linear regression line for the points X in red\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 6- Linear regression visualization with a complex data and degree 4 features used to train linear regression model\n\n\n\n\nSource: linear_non_linear_regression_final.ipynb\nWe can observe after generating degree 4 polynomial features linear regression has fitted our complex function relativey well.\n\n\nConlusion-\nIn this blog we have learnt about linear and non linear regression particularly polynomial regression and its use with different kinds of data.\n\n\nPython notebook code link in the Github repository\nhttps://github.com/rahulsatoskar/CS-5805-Machine-Learning-Blogs/blob/main/Python%20notebooks/linear_non_linear_regression_final.ipynb"
  },
  {
    "objectID": "posts/linear and non linear regression/linear_non_linear_regression_final.html",
    "href": "posts/linear and non linear regression/linear_non_linear_regression_final.html",
    "title": "CS-5805-Machine-Learning-Blogs",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = np.random.rand(150,1)\n\n# Let us generate a function y which is a linear function of X with added Gaussian noise\ny = 10 * X + 5 + np.random.randn(150, 1)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X, y)\n\n# https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\nX1 = np.linspace(0, 1, 150).reshape(-1, 1)\ny1_pred = LR.predict(X1)\n\nplt.scatter(X, y)\n# Let us plot a linear regression line for the points X in red\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 1- Linear regression visualization\n\n\n\n\n\n# Adding more noise to our data\n# Linear regression visualization with more noise in our data\n\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = np.random.rand(150,1)\n\n# Let us generate a function y which is a linear function of X with added Gaussian noise\ny = 10 * X + 5 + 5 * np.random.randn(150, 1)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X, y)\n\nX1 = np.linspace(0, 1, 150).reshape(-1, 1)\ny1_pred = LR.predict(X1)\n\nplt.scatter(X, y)\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 2- Linear regression visualization with more noise\n\n\n\n\n\n# Adding less noise to our data\n# Linear regression visualization with less noise in our data\n\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = np.random.rand(150,1)\n\n# Let us generate a function y which is a linear function of X with added Gaussian noise\ny = 10 * X + 5 + 0.2 * np.random.randn(150, 1)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X, y)\n\nX1 = np.linspace(0, 1, 150).reshape(-1, 1)\ny1_pred = LR.predict(X1)\n\nplt.scatter(X, y)\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 3- Linear regression visualization with less noise\n\n\n\n\n\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = 4 * np.random.rand(80,1) - 2\n\n# Let us generate a function y which is a quadratic function of X with added Gaussian noise\ny = 2 * X + 5 + X**2 + np.random.randn(80, 1)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X, y)\n\n# https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\nX1 = np.linspace(-2, 2, 80).reshape(-1, 1)\ny1_pred = LR.predict(X1)\n\nplt.scatter(X, y)\n# Let us plot a linear regression line for the points X in red\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 4- Linear regression visualization with quadratic data\n\n\n\n\nObservations- We can see a quadratic curve and the best fit of line determined by linear regression model. As we can see a line does not do a good job in fitting a quadratic curve amd hence a linear model like linear regression is not good for fitting such a quadratic data.\nThis is why we want to have polynomial regression to fit such a data. For example quadratic regression would be able to fit such a quadratic data.\nFor such a non linear data like the quadratic data as shown above we use polynomial features where we take the features which we have and we create polynomial features based on those features.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = 4 * np.random.rand(80,1) - 2\n\n# Let us generate a function y which is a quadratic function of X with added Gaussian noise\ny = 2 * X + 5 + X**2 + np.random.randn(80, 1)\n\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X_poly, y)\n\n# https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\nX1 = np.linspace(-2, 2, 80).reshape(-1, 1)\nX1_poly = poly.fit_transform(X1)\n\ny1_pred = LR.predict(X1_poly)\n\nplt.scatter(X, y)\n# Let us plot a linear regression line for the points X in red\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 5- Linear regression visualization with quadratic data and quadratic features used to train linear regression model\n\n\n\n\nNow we can see we have a quadratic function to fit these points which does a better job than linear regression.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n# https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html\n\n# Create an array of the given shape and populate it with random samples from a uniform distribution over [0, 1).\nX = 4 * np.random.rand(80,1) - 2\n\n# Let us generate a function y which has degree 1, quadratic, cubic and degree 4 terms of X with added Gaussian noise\ny = 2 * X + 5 + X**2 + 10 * X**3 + 2 * X**4 + 5 * np.random.randn(80, 1)\n\npoly = PolynomialFeatures(degree=4, include_bias=False)\nX_poly = poly.fit_transform(X)\n\n# Fitting a linear regression model to the generated dataset\nLR = LinearRegression()\nLR.fit(X_poly, y)\n\n# https://numpy.org/doc/stable/reference/generated/numpy.linspace.html\nX1 = np.linspace(-2, 2, 80).reshape(-1, 1)\nX1_poly = poly.fit_transform(X1)\n\ny1_pred = LR.predict(X1_poly)\n\nplt.scatter(X, y)\n# Let us plot a linear regression line for the points X in red\nplt.plot(X1, y1_pred, color = 'r')\nplt.show()\n\n\n\n\nFig 6- Linear regression visualization with a complex data and degree 4 features used to train linear regression model\n\n\n\n\nObservations- We can observe after generating degree 4 polynomial features we have fit our complex function relativey well."
  },
  {
    "objectID": "posts/Linear-regression-assumptions/index.html",
    "href": "posts/Linear-regression-assumptions/index.html",
    "title": "Assumptions in Linear Regression",
    "section": "",
    "text": "In this blog we will talk about the assumptions of simple linear regression. There are 4 assumptions of simple linear regression they are:\n1- Linear Relationship between the input and output\n2- No Multicollinearity\n3- Normality of Residual\n4- Homoscedasticity\nWe will check the assumptions of simple linear regression using the California housing Data set. Let us download the dataset as shown below.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from sklearn import datasets\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# Loading pre-defined California housing Dataset\ncalifornia_housing = fetch_california_housing(as_frame=True)\n\n# Load the dataset into Pandas Dataframe\ncalifornia_housing_data = pd.DataFrame(california_housing.frame)\ncalifornia_housing_data\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20635\n1.5603\n25.0\n5.045455\n1.133333\n845.0\n2.560606\n39.48\n-121.09\n0.781\n\n\n20636\n2.5568\n18.0\n6.114035\n1.315789\n356.0\n3.122807\n39.49\n-121.21\n0.771\n\n\n20637\n1.7000\n17.0\n5.205543\n1.120092\n1007.0\n2.325635\n39.43\n-121.22\n0.923\n\n\n20638\n1.8672\n18.0\n5.329513\n1.171920\n741.0\n2.123209\n39.43\n-121.32\n0.847\n\n\n20639\n2.3886\n16.0\n5.254717\n1.162264\n1387.0\n2.616981\n39.37\n-121.24\n0.894\n\n\n\n\n20640 rows × 9 columns\n\nFig 1- California housing Dataset\n\n\nSource: linear_regression_assumptions_Final.ipynb\nLet us now split the data into input features and target variables and shown below.\n\n\n# Splitting data into input features and target variables\nX = california_housing_data.iloc[:,0:8].values\ny = california_housing_data.iloc[:,-1].values\n\n# Standardizing input data using Standard Scaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nSource: linear_regression_assumptions_Final.ipynb\nLet us now split the data into training and test data sets.\n\n\n# Train test split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=20)\n\nSource: linear_regression_assumptions_Final.ipynb\nLet us now train a linear regression model on our training data set as shown below.\n\n\n# Training linear regression model on train data\nLR = LinearRegression()\n\nLR.fit(X_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nSource: linear_regression_assumptions_Final.ipynb\nLet us now find residuals which are the difference between the actual target value and the target value predicted by the linear regression model.\n\n\n# Finding Residuals which are the difference between the actual target value and the target value predicted\n# by the linear regression model\n\ny_pred = LR.predict(X_test)\nresidual = y_test - y_pred\n\nSource: linear_regression_assumptions_Final.ipynb\n\n1- To check Linear relationship between input and output variables\nWe plot scatter plots between each of the input features and the target variable to check for linearity as shown below.\n\n\nfig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8) = plt.subplots(ncols=8, figsize=(15, 5))\n\nax1.scatter(california_housing_data['MedInc'], california_housing_data['MedHouseVal'])\nax1.set_title(\"MedInc\")\nax2.scatter(california_housing_data['HouseAge'], california_housing_data['MedHouseVal'])\nax2.set_title(\"HouseAge\")\nax3.scatter(california_housing_data['AveRooms'], california_housing_data['MedHouseVal'])\nax3.set_title(\"AveRooms\")\nax4.scatter(california_housing_data['AveBedrms'], california_housing_data['MedHouseVal'])\nax4.set_title(\"AveBedrms\")\nax5.scatter(california_housing_data['Population'], california_housing_data['MedHouseVal'])\nax5.set_title(\"Population\")\nax6.scatter(california_housing_data['AveOccup'], california_housing_data['MedHouseVal'])\nax6.set_title(\"AveOccup\")\nax7.scatter(california_housing_data['Latitude'], california_housing_data['MedHouseVal'])\nax7.set_title(\"Latitude\")\nax8.scatter(california_housing_data['Longitude'], california_housing_data['MedHouseVal'])\nax8.set_title(\"Longitude\")\n\n\nplt.show()\n\n\n\n\nFig 2- Linear relationship test between input and output variables\n\n\n\n\nSource: linear_regression_assumptions_Final.ipynb\nObservations- From the above plots we can see that none of the input features have a strong linear relationship with the target variable. The feature MedInc does have some linear relationship with the target variable but not a strong linear relationship.\nWe can make transformations to the input features like log transformation,power transformation etc so that we meet this assumption.\n\n\n2- Multicollinearity check\nVariance Inflation Factor (VIF)\nThere are several ways to detect multicollinearity in a dataset. One such technique is called the Variance Inflation Factor (VIF).\nVIF determines the strength of the correlation between the independent variables or the input features. VIF is predicted by taking a variable and regressing it against every other variable.\nVIF score of an independent variable represents how well the variable is explained by other independent variables. VIF has the following properties:\na- VIF can takes from 1 and has no upper limit.\nb- If VIF = 1 there is no correlation between the independent variable and the other variables.\nc- VIF exceeding 5 or 10 indicates high multicollinearity between the independent variable and the other independent variables.\nLet us check the VIF score of our independent variables as shown below:\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Python list to store VIF for each of the features\nvif = []\n\nfor i in range(X_train.shape[1]):\n    vif.append(variance_inflation_factor(X_train, i))\n    \npd.DataFrame({'vif': vif}, index=california_housing_data.columns[0:8]).T\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\nvif\n2.592546\n1.247793\n8.616084\n7.07929\n1.144284\n1.011484\n9.379948\n9.010217\n\n\n\n\n\n\n\nSource: linear_regression_assumptions_Final.ipynb\nObservations- We can see that the features AveRooms, AveBedrms, Latitude and Longitude have a high VIF value and hence they can be predicted by other independent variables in the dataset. Hence multicollinearity exists in our dataset.\n\n\n3- Checking normality of residuals\nLet us check if our residuals follow a normal distribution.\n\n\nsns.displot(residual,kind='kde')\n\n\n\n\nFig 3- Checking normality of residuals\n\n\n\n\nSource: linear_regression_assumptions_Final.ipynb\nObservations- From the above plot we can see that the residuals are approximately normally distributed and the assumption of normality of residuals holds.\nLet us now check the normality of residuals using a QQ plot.\n\n\n# QQ Plot\n\nimport scipy as sp\n\nfig, ax = plt.subplots(figsize=(7,5))\nsp.stats.probplot(residual, plot=ax, fit=True)\n\nplt.show()\n\n\n\n\nFig 4- Checking normality of residuals using QQ plot\n\n\n\n\nSource: linear_regression_assumptions_Final.ipynb\nObservations- The normal quantile plot is obtained by plotting the residuals against theoretical quantiles of the standard normal distribution. From the above plot we can see that the residuals are close to the 45 degrees line and hence the residuals can be considered to be from normal distribution.\n\n\n4- Homoscedasticity (Equal variances): Residuals have constant variance across the values of the dependent variables.\nLet us check the assumption of homoscedasticity as shown below\n\n\nplt.scatter(y_pred,residual)\n\n&lt;matplotlib.collections.PathCollection at 0x276e90459a0&gt;\nFig 5- Checking assumption of Homoscedasticity\n\n\n\n\n\nSource: linear_regression_assumptions_Final.ipynb\nObservations- To check Homoscedasticity we plot residuals against predicted values. If we see a haphazard cloud of points our homoscedasticity assumption is met else if we see patterns it suggests non-linearity and/or heteroscedasticity (unequal variances).\nFrom the above plot we can see that our Homoscedasticity assumption of linear regression has not met and we have heteroscedasticity (unequal variances).\n\n\n5- Conclusion\nIn this blog we checked the assumptions of simple linear regression on a California housing dataset.\n\n\n6- Python notebook code link in the Github repository\nhttps://github.com/rahulsatoskar/CS-5805-Machine-Learning-Blogs/blob/main/Python%20notebooks/linear_regression_assumptions_Final.ipynb"
  },
  {
    "objectID": "posts/Linear-regression-assumptions/linear_regression_assumptions_Final.html",
    "href": "posts/Linear-regression-assumptions/linear_regression_assumptions_Final.html",
    "title": "CS-5805-Machine-Learning-Blogs",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from sklearn import datasets\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\n# Loading pre-defined California housing Dataset\ncalifornia_housing = fetch_california_housing(as_frame=True)\n\n# Load the dataset into Pandas Dataframe\ncalifornia_housing_data = pd.DataFrame(california_housing.frame)\ncalifornia_housing_data\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20635\n1.5603\n25.0\n5.045455\n1.133333\n845.0\n2.560606\n39.48\n-121.09\n0.781\n\n\n20636\n2.5568\n18.0\n6.114035\n1.315789\n356.0\n3.122807\n39.49\n-121.21\n0.771\n\n\n20637\n1.7000\n17.0\n5.205543\n1.120092\n1007.0\n2.325635\n39.43\n-121.22\n0.923\n\n\n20638\n1.8672\n18.0\n5.329513\n1.171920\n741.0\n2.123209\n39.43\n-121.32\n0.847\n\n\n20639\n2.3886\n16.0\n5.254717\n1.162264\n1387.0\n2.616981\n39.37\n-121.24\n0.894\n\n\n\n\n20640 rows × 9 columns\n\nFig 1- California housing Dataset\n\n\n\n# Splitting data into input features and target variables\nX = california_housing_data.iloc[:,0:8].values\ny = california_housing_data.iloc[:,-1].values\n\n# Standardizing input data using Standard Scaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\n# Train test split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=20)\n\n\n# Training linear regression model on train data\nLR = LinearRegression()\n\nLR.fit(X_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# Finding Residuals which are the difference between the actual target value and the target value predicted\n# by the linear regression model\n\ny_pred = LR.predict(X_test)\nresidual = y_test - y_pred\n\n1- To check Linear relationship between input and output variables\n\nfig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8) = plt.subplots(ncols=8, figsize=(15, 5))\n\nax1.scatter(california_housing_data['MedInc'], california_housing_data['MedHouseVal'])\nax1.set_title(\"MedInc\")\nax2.scatter(california_housing_data['HouseAge'], california_housing_data['MedHouseVal'])\nax2.set_title(\"HouseAge\")\nax3.scatter(california_housing_data['AveRooms'], california_housing_data['MedHouseVal'])\nax3.set_title(\"AveRooms\")\nax4.scatter(california_housing_data['AveBedrms'], california_housing_data['MedHouseVal'])\nax4.set_title(\"AveBedrms\")\nax5.scatter(california_housing_data['Population'], california_housing_data['MedHouseVal'])\nax5.set_title(\"Population\")\nax6.scatter(california_housing_data['AveOccup'], california_housing_data['MedHouseVal'])\nax6.set_title(\"AveOccup\")\nax7.scatter(california_housing_data['Latitude'], california_housing_data['MedHouseVal'])\nax7.set_title(\"Latitude\")\nax8.scatter(california_housing_data['Longitude'], california_housing_data['MedHouseVal'])\nax8.set_title(\"Longitude\")\n\n\nplt.show()\n\n\n\n\nFig 2- Linear relationship test between input and output variables\n\n\n\n\nNote- From the above plots we can see that none of the input features have a strong linear relationship with the target variable. The feature MedInc does have some linear relationship with the target variable but not a strong linear relationship.\nWe can make transformations to the input features like log transformation,power transformation etc so that we meet this assumption.\n2- Multicollinearity check\nVariance Inflation Factor (VIF)\nThere are several ways to detect multicollinearity in a dataset. One such technique is called the Variance Inflation Factor (VIF).\nVIF determines the strength of the correlation between the independent variables or the input features. VIF is predicted by taking a variable and regressing it against every other variable.\nVIF score of an independent variable represents how well the variable is explained by other independent variables.\nVIF can takes from 1 and has no upper limit.\nIf VIF = 1 there is no correlation between the independent variable and the other variables.\nVIF exceeding 5 or 10 indicates high multicollinearity between the independent variable and the other independent variables.\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Python list to store VIF for each of the features\nvif = []\n\nfor i in range(X_train.shape[1]):\n    vif.append(variance_inflation_factor(X_train, i))\n    \npd.DataFrame({'vif': vif}, index=california_housing_data.columns[0:8]).T\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\nvif\n2.592546\n1.247793\n8.616084\n7.07929\n1.144284\n1.011484\n9.379948\n9.010217\n\n\n\n\n\n\n\nNote- We can see that the features AveRooms, AveBedrms, Latitude and Longitude have a high VIF value and hence they can be predicted by other independent variables in the dataset. Hence multicollinearity exists in our dataset.\n3- Checking normality of residuals\n\nsns.displot(residual,kind='kde')\n\n\n\n\nFig 3- Checking normality of residuals\n\n\n\n\nNote from the above plot we can see that the residuals are approxiamately normally distributed and the assumption of normality os residuals holds.\n\n# QQ Plot\n\nimport scipy as sp\n\nfig, ax = plt.subplots(figsize=(7,5))\nsp.stats.probplot(residual, plot=ax, fit=True)\n\nplt.show()\n\n\n\n\nFig 4- Checking normality of residuals using QQ plot\n\n\n\n\nThe normal quantile plot is obtained byplotting residuals against theoretical quantiles of the standard normal distribution. From the above plot we can see that the residuals are close to the 45 degrees line and hence the residuals can be considered to be from normal distribution.\n4- Homoscedasticity (Equal variances): Residuals have constant variance across the values of the dependent variables.\n\nplt.scatter(y_pred,residual)\n\n&lt;matplotlib.collections.PathCollection at 0x276e90459a0&gt;\nFig 5- Checking assumption of Homoscedasticity\n\n\n\n\n\nObservations- To check Homoscedasticity we plot residuals against predicted values. If we see a haphazard cloud of points our homoscedasticity assumption is met else if we see patterns it suggests non-linearity and/or heteroscedasticity (unequal variances).\nFrom the above plot we can see that our Homoscedasticity assumption of linear regression has not met and we have heteroscedasticity (unequal variances)."
  }
]
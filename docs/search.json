[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Rahul Satoskar"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS-5805-Machine-Learning-Blogs",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAssumptions in Linear Regression\n\n\n\n\n\n\n\nProbability theory and random variables\n\n\nLinear regression\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nRahul Satoskar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Linear-regression-assumptions/index.html",
    "href": "posts/Linear-regression-assumptions/index.html",
    "title": "Assumptions in Linear Regression",
    "section": "",
    "text": "In this blog we will talk about the assumptions of simple linear regression. There are 4 assumptions of simple linear regression they are:\n1- Linear Relationship between the input and output\n2- No Multicollinearity\n3- Normality of Residual\n4- Homoscedasticity\nWe will check the assumptions of simple linear regression using the California housing Data set. Let us download the dataset as shown below.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from sklearn import datasets\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Loading pre-defined California housing Dataset\ncalifornia_housing = fetch_california_housing(as_frame=True)\n\n# Load the dataset into Pandas Dataframe\ncalifornia_housing_data = pd.DataFrame(california_housing.frame)\ncalifornia_housing_data\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20635\n1.5603\n25.0\n5.045455\n1.133333\n845.0\n2.560606\n39.48\n-121.09\n0.781\n\n\n20636\n2.5568\n18.0\n6.114035\n1.315789\n356.0\n3.122807\n39.49\n-121.21\n0.771\n\n\n20637\n1.7000\n17.0\n5.205543\n1.120092\n1007.0\n2.325635\n39.43\n-121.22\n0.923\n\n\n20638\n1.8672\n18.0\n5.329513\n1.171920\n741.0\n2.123209\n39.43\n-121.32\n0.847\n\n\n20639\n2.3886\n16.0\n5.254717\n1.162264\n1387.0\n2.616981\n39.37\n-121.24\n0.894\n\n\n\n\n20640 rows Ã— 9 columns\n\nFig 1- California housing Dataset\n\n\nSource: linear_regression_assumptions_Final.ipynb\nLet us now split the data into input features and target variables and shown below.\n\n\n# Splitting data into input features and target variables\nX = california_housing_data.iloc[:,0:8].values\ny = california_housing_data.iloc[:,-1].values\n\n# Standardizing input data using Standard Scaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\nSource: linear_regression_assumptions_Final.ipynb\nLet us now split the data into training and test data sets.\n\n\n# Train test split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=20)\n\nSource: linear_regression_assumptions_Final.ipynb\nLet us now train a linear regression model on our training data set as shown below.\n\n\n# Training linear regression model on train data\nLR = LinearRegression()\n\nLR.fit(X_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nSource: linear_regression_assumptions_Final.ipynb\nLet us now find residuals which are the difference between the actual target value and the target value predicted by the linear regression model.\n\n\n# Finding Residuals which are the difference between the actual target value and the target value predicted\n# by the linear regression model\n\ny_pred = LR.predict(X_test)\nresidual = y_test - y_pred\n\nSource: linear_regression_assumptions_Final.ipynb\n\n1- To check Linear relationship between input and output variables\nWe plot scatter plots between each of the input features and the target variable to check for linearity as shown below.\n\n\nfig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8) = plt.subplots(ncols=8, figsize=(15, 5))\n\nax1.scatter(california_housing_data['MedInc'], california_housing_data['MedHouseVal'])\nax1.set_title(\"MedInc\")\nax2.scatter(california_housing_data['HouseAge'], california_housing_data['MedHouseVal'])\nax2.set_title(\"HouseAge\")\nax3.scatter(california_housing_data['AveRooms'], california_housing_data['MedHouseVal'])\nax3.set_title(\"AveRooms\")\nax4.scatter(california_housing_data['AveBedrms'], california_housing_data['MedHouseVal'])\nax4.set_title(\"AveBedrms\")\nax5.scatter(california_housing_data['Population'], california_housing_data['MedHouseVal'])\nax5.set_title(\"Population\")\nax6.scatter(california_housing_data['AveOccup'], california_housing_data['MedHouseVal'])\nax6.set_title(\"AveOccup\")\nax7.scatter(california_housing_data['Latitude'], california_housing_data['MedHouseVal'])\nax7.set_title(\"Latitude\")\nax8.scatter(california_housing_data['Longitude'], california_housing_data['MedHouseVal'])\nax8.set_title(\"Longitude\")\n\n\nplt.show()\n\n\n\n\nFig 2- Linear relationship test between input and output variables\n\n\n\n\nSource: linear_regression_assumptions_Final.ipynb\nObservations- From the above plots we can see that none of the input features have a strong linear relationship with the target variable. The feature MedInc does have some linear relationship with the target variable but not a strong linear relationship.\nWe can make transformations to the input features like log transformation,power transformation etc so that we meet this assumption.\n\n\n2- Multicollinearity check\nVariance Inflation Factor (VIF)\nThere are several ways to detect multicollinearity in a dataset. One such technique is called the Variance Inflation Factor (VIF).\nVIF determines the strength of the correlation between the independent variables or the input features. VIF is predicted by taking a variable and regressing it against every other variable.\nVIF score of an independent variable represents how well the variable is explained by other independent variables. VIF has the following properties:\na- VIF can takes from 1 and has no upper limit.\nb- If VIF = 1 there is no correlation between the independent variable and the other variables.\nc- VIF exceeding 5 or 10 indicates high multicollinearity between the independent variable and the other independent variables.\nLet us check the VIF score of our independent variables as shown below:\n\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Python list to store VIF for each of the features\nvif = []\n\nfor i in range(X_train.shape[1]):\n    vif.append(variance_inflation_factor(X_train, i))\n    \npd.DataFrame({'vif': vif}, index=california_housing_data.columns[0:8]).T\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\nvif\n2.592546\n1.247793\n8.616084\n7.07929\n1.144284\n1.011484\n9.379948\n9.010217\n\n\n\n\n\n\n\nSource: linear_regression_assumptions_Final.ipynb\nObservations- We can see that the features AveRooms, AveBedrms, Latitude and Longitude have a high VIF value and hence they can be predicted by other independent variables in the dataset. Hence multicollinearity exists in our dataset.\n\n\n3- Checking normality of residuals\nLet us check if our residuals follow a normal distribution.\n\n\nsns.displot(residual,kind='kde')\n\n\n\n\nFig 3- Checking normality of residuals\n\n\n\n\nSource: linear_regression_assumptions_Final.ipynb\nObservations- From the above plot we can see that the residuals are approximately normally distributed and the assumption of normality of residuals holds.\nLet us now check the normality of residuals using a QQ plot.\n\n\n# QQ Plot\n\nimport scipy as sp\n\nfig, ax = plt.subplots(figsize=(7,5))\nsp.stats.probplot(residual, plot=ax, fit=True)\n\nplt.show()\n\n\n\n\nFig 4- Checking normality of residuals using QQ plot\n\n\n\n\nSource: linear_regression_assumptions_Final.ipynb\nObservations- The normal quantile plot is obtained by plotting the residuals against theoretical quantiles of the standard normal distribution. From the above plot we can see that the residuals are close to the 45 degrees line and hence the residuals can be considered to be from normal distribution.\n\n\n4- Homoscedasticity (Equal variances): Residuals have constant variance across the values of the dependent variables.\nLet us check the assumption of homoscedasticity as shown below\n\n\nplt.scatter(y_pred,residual)\n\n&lt;matplotlib.collections.PathCollection at 0x209d4d050a0&gt;\nFig 4- Checking assumption of Homoscedasticity\n\n\n\n\n\nSource: linear_regression_assumptions_Final.ipynb\nObservations- To check Homoscedasticity we plot residuals against predicted values. If we see a haphazard cloud of points our homoscedasticity assumption is met else if we see patterns it suggests non-linearity and/or heteroscedasticity (unequal variances).\nFrom the above plot we can see that our Homoscedasticity assumption of linear regression has not met and we have heteroscedasticity (unequal variances).\n\n\n5- Conclusion\nIn this blog we checked the assumptions of simple linear regression on a California housing dataset.\n\n\n6- Python notebook code link in the Github repository\nhttps://github.com/rahulsatoskar/CS-5805-Machine-Learning-Blogs/blob/main/Python%20notebooks/linear_regression_assumptions_Final.ipynb"
  },
  {
    "objectID": "posts/Linear-regression-assumptions/linear_regression_assumptions_Final.html",
    "href": "posts/Linear-regression-assumptions/linear_regression_assumptions_Final.html",
    "title": "CS-5805-Machine-Learning-Blogs",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from sklearn import datasets\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Loading pre-defined California housing Dataset\ncalifornia_housing = fetch_california_housing(as_frame=True)\n\n# Load the dataset into Pandas Dataframe\ncalifornia_housing_data = pd.DataFrame(california_housing.frame)\ncalifornia_housing_data\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20635\n1.5603\n25.0\n5.045455\n1.133333\n845.0\n2.560606\n39.48\n-121.09\n0.781\n\n\n20636\n2.5568\n18.0\n6.114035\n1.315789\n356.0\n3.122807\n39.49\n-121.21\n0.771\n\n\n20637\n1.7000\n17.0\n5.205543\n1.120092\n1007.0\n2.325635\n39.43\n-121.22\n0.923\n\n\n20638\n1.8672\n18.0\n5.329513\n1.171920\n741.0\n2.123209\n39.43\n-121.32\n0.847\n\n\n20639\n2.3886\n16.0\n5.254717\n1.162264\n1387.0\n2.616981\n39.37\n-121.24\n0.894\n\n\n\n\n20640 rows Ã— 9 columns\n\nFig 1- California housing Dataset\n\n\n\n# Splitting data into input features and target variables\nX = california_housing_data.iloc[:,0:8].values\ny = california_housing_data.iloc[:,-1].values\n\n# Standardizing input data using Standard Scaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\n# Train test split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=20)\n\n\n# Training linear regression model on train data\nLR = LinearRegression()\n\nLR.fit(X_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# Finding Residuals which are the difference between the actual target value and the target value predicted\n# by the linear regression model\n\ny_pred = LR.predict(X_test)\nresidual = y_test - y_pred\n\n1- To check Linear relationship between input and output variables\n\nfig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8) = plt.subplots(ncols=8, figsize=(15, 5))\n\nax1.scatter(california_housing_data['MedInc'], california_housing_data['MedHouseVal'])\nax1.set_title(\"MedInc\")\nax2.scatter(california_housing_data['HouseAge'], california_housing_data['MedHouseVal'])\nax2.set_title(\"HouseAge\")\nax3.scatter(california_housing_data['AveRooms'], california_housing_data['MedHouseVal'])\nax3.set_title(\"AveRooms\")\nax4.scatter(california_housing_data['AveBedrms'], california_housing_data['MedHouseVal'])\nax4.set_title(\"AveBedrms\")\nax5.scatter(california_housing_data['Population'], california_housing_data['MedHouseVal'])\nax5.set_title(\"Population\")\nax6.scatter(california_housing_data['AveOccup'], california_housing_data['MedHouseVal'])\nax6.set_title(\"AveOccup\")\nax7.scatter(california_housing_data['Latitude'], california_housing_data['MedHouseVal'])\nax7.set_title(\"Latitude\")\nax8.scatter(california_housing_data['Longitude'], california_housing_data['MedHouseVal'])\nax8.set_title(\"Longitude\")\n\n\nplt.show()\n\n\n\n\nFig 2- Linear relationship test between input and output variables\n\n\n\n\nNote- From the above plots we can see that none of the input features have a strong linear relationship with the target variable. The feature MedInc does have some linear relationship with the target variable but not a strong linear relationship.\nWe can make transformations to the input features like log transformation,power transformation etc so that we meet this assumption.\n2- Multicollinearity check\nVariance Inflation Factor (VIF)\nThere are several ways to detect multicollinearity in a dataset. One such technique is called the Variance Inflation Factor (VIF).\nVIF determines the strength of the correlation between the independent variables or the input features. VIF is predicted by taking a variable and regressing it against every other variable.\nVIF score of an independent variable represents how well the variable is explained by other independent variables.\nVIF can takes from 1 and has no upper limit.\nIf VIF = 1 there is no correlation between the independent variable and the other variables.\nVIF exceeding 5 or 10 indicates high multicollinearity between the independent variable and the other independent variables.\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Python list to store VIF for each of the features\nvif = []\n\nfor i in range(X_train.shape[1]):\n    vif.append(variance_inflation_factor(X_train, i))\n    \npd.DataFrame({'vif': vif}, index=california_housing_data.columns[0:8]).T\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\nvif\n2.592546\n1.247793\n8.616084\n7.07929\n1.144284\n1.011484\n9.379948\n9.010217\n\n\n\n\n\n\n\nNote- We can see that the features AveRooms, AveBedrms, Latitude and Longitude have a high VIF value and hence they can be predicted by other independent variables in the dataset. Hence multicollinearity exists in our dataset.\n3- Checking normality of residuals\n\nsns.displot(residual,kind='kde')\n\n\n\n\nFig 3- Checking normality of residuals\n\n\n\n\nNote from the above plot we can see that the residuals are approxiamately normally distributed and the assumption of normality os residuals holds.\n\n# QQ Plot\n\nimport scipy as sp\n\nfig, ax = plt.subplots(figsize=(7,5))\nsp.stats.probplot(residual, plot=ax, fit=True)\n\nplt.show()\n\n\n\n\nFig 4- Checking normality of residuals using QQ plot\n\n\n\n\nThe normal quantile plot is obtained byplotting residuals against theoretical quantiles of the standard normal distribution. From the above plot we can see that the residuals are close to the 45 degrees line and hence the residuals can be considered to be from normal distribution.\n4- Homoscedasticity (Equal variances): Residuals have constant variance across the values of the dependent variables.\n\nplt.scatter(y_pred,residual)\n\n&lt;matplotlib.collections.PathCollection at 0x209d4d050a0&gt;\nFig 4- Checking assumption of Homoscedasticity\n\n\n\n\n\nObservations- To check Homoscedasticity we plot residuals against predicted values. If we see a haphazard cloud of points our homoscedasticity assumption is met else if we see patterns it suggests non-linearity and/or heteroscedasticity (unequal variances).\nFrom the above plot we can see that our Homoscedasticity assumption of linear regression has not met and we have heteroscedasticity (unequal variances)."
  },
  {
    "objectID": "docs/posts/Linear-regression-assumptions/linear_regression_assumptions_Final.html",
    "href": "docs/posts/Linear-regression-assumptions/linear_regression_assumptions_Final.html",
    "title": "CS-5805-Machine-Learning-Blogs",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# from sklearn import datasets\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\n\n# Loading pre-defined California housing Dataset\ncalifornia_housing = fetch_california_housing(as_frame=True)\n\n# Load the dataset into Pandas Dataframe\ncalifornia_housing_data = pd.DataFrame(california_housing.frame)\ncalifornia_housing_data\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20635\n1.5603\n25.0\n5.045455\n1.133333\n845.0\n2.560606\n39.48\n-121.09\n0.781\n\n\n20636\n2.5568\n18.0\n6.114035\n1.315789\n356.0\n3.122807\n39.49\n-121.21\n0.771\n\n\n20637\n1.7000\n17.0\n5.205543\n1.120092\n1007.0\n2.325635\n39.43\n-121.22\n0.923\n\n\n20638\n1.8672\n18.0\n5.329513\n1.171920\n741.0\n2.123209\n39.43\n-121.32\n0.847\n\n\n20639\n2.3886\n16.0\n5.254717\n1.162264\n1387.0\n2.616981\n39.37\n-121.24\n0.894\n\n\n\n\n20640 rows Ã— 9 columns\n\nFig 1- California housing Dataset\n\n\n\n# Splitting data into input features and target variables\nX = california_housing_data.iloc[:,0:8].values\ny = california_housing_data.iloc[:,-1].values\n\n# Standardizing input data using Standard Scaler\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n\n# Train test split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=20)\n\n\n# Training linear regression model on train data\nLR = LinearRegression()\n\nLR.fit(X_train,y_train)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n# Finding Residuals which are the difference between the actual target value and the target value predicted\n# by the linear regression model\n\ny_pred = LR.predict(X_test)\nresidual = y_test - y_pred\n\n1- To check Linear relationship between input and output variables\n\nfig, (ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8) = plt.subplots(ncols=8, figsize=(15, 5))\n\nax1.scatter(california_housing_data['MedInc'], california_housing_data['MedHouseVal'])\nax1.set_title(\"MedInc\")\nax2.scatter(california_housing_data['HouseAge'], california_housing_data['MedHouseVal'])\nax2.set_title(\"HouseAge\")\nax3.scatter(california_housing_data['AveRooms'], california_housing_data['MedHouseVal'])\nax3.set_title(\"AveRooms\")\nax4.scatter(california_housing_data['AveBedrms'], california_housing_data['MedHouseVal'])\nax4.set_title(\"AveBedrms\")\nax5.scatter(california_housing_data['Population'], california_housing_data['MedHouseVal'])\nax5.set_title(\"Population\")\nax6.scatter(california_housing_data['AveOccup'], california_housing_data['MedHouseVal'])\nax6.set_title(\"AveOccup\")\nax7.scatter(california_housing_data['Latitude'], california_housing_data['MedHouseVal'])\nax7.set_title(\"Latitude\")\nax8.scatter(california_housing_data['Longitude'], california_housing_data['MedHouseVal'])\nax8.set_title(\"Longitude\")\n\n\nplt.show()\n\n\n\n\nFig 2- Linear relationship test between input and output variables\n\n\n\n\nNote- From the above plots we can see that none of the input features have a strong linear relationship with the target variable. The feature MedInc does have some linear relationship with the target variable but not a strong linear relationship.\nWe can make transformations to the input features like log transformation,power transformation etc so that we meet this assumption.\n2- Multicollinearity check\nVariance Inflation Factor (VIF)\nThere are several ways to detect multicollinearity in a dataset. One such technique is called the Variance Inflation Factor (VIF).\nVIF determines the strength of the correlation between the independent variables or the input features. VIF is predicted by taking a variable and regressing it against every other variable.\nVIF score of an independent variable represents how well the variable is explained by other independent variables.\nVIF can takes from 1 and has no upper limit.\nIf VIF = 1 there is no correlation between the independent variable and the other variables.\nVIF exceeding 5 or 10 indicates high multicollinearity between the independent variable and the other independent variables.\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Python list to store VIF for each of the features\nvif = []\n\nfor i in range(X_train.shape[1]):\n    vif.append(variance_inflation_factor(X_train, i))\n    \npd.DataFrame({'vif': vif}, index=california_housing_data.columns[0:8]).T\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\n\n\n\n\nvif\n2.592546\n1.247793\n8.616084\n7.07929\n1.144284\n1.011484\n9.379948\n9.010217\n\n\n\n\n\n\n\nNote- We can see that the features AveRooms, AveBedrms, Latitude and Longitude have a high VIF value and hence they can be predicted by other independent variables in the dataset. Hence multicollinearity exists in our dataset.\n3- Checking normality of residuals\n\nsns.displot(residual,kind='kde')\n\n\n\n\nFig 3- Checking normality of residuals\n\n\n\n\nNote from the above plot we can see that the residuals are approxiamately normally distributed and the assumption of normality os residuals holds.\n\n# QQ Plot\n\nimport scipy as sp\n\nfig, ax = plt.subplots(figsize=(7,5))\nsp.stats.probplot(residual, plot=ax, fit=True)\n\nplt.show()\n\n\n\n\nFig 4- Checking normality of residuals using QQ plot\n\n\n\n\nThe normal quantile plot is obtained byplotting residuals against theoretical quantiles of the standard normal distribution. From the above plot we can see that the residuals are close to the 45 degrees line and hence the residuals can be considered to be from normal distribution.\n4- Homoscedasticity (Equal variances): Residuals have constant variance across the values of the dependent variables.\n\nplt.scatter(y_pred,residual)\n\n&lt;matplotlib.collections.PathCollection at 0x209d4d050a0&gt;\nFig 4- Checking assumption of Homoscedasticity\n\n\n\n\n\nObservations- To check Homoscedasticity we plot residuals against predicted values. If we see a haphazard cloud of points our homoscedasticity assumption is met else if we see patterns it suggests non-linearity and/or heteroscedasticity (unequal variances).\nFrom the above plot we can see that our Homoscedasticity assumption of linear regression has not met and we have heteroscedasticity (unequal variances)."
  }
]